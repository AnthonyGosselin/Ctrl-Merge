{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_num = 8 # {8: 1094,  0: 0061, 9: 1100}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.399 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "from nuscenes.nuscenes import NuScenes\n",
    "from nuscenes.map_expansion.map_api import NuScenesMap\n",
    "\n",
    "from nuscenes.utils.geometry_utils import box_in_image, view_points\n",
    "from nuscenes.utils.data_classes import Box\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.patches as patches\n",
    "from pyquaternion import Quaternion\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "nusc = NuScenes(version='v1.0-mini', dataroot='/network/scratch/a/anthony.gosselin/nuscenes', verbose=True)\n",
    "nusc_map = NuScenesMap(dataroot='/network/scratch/a/anthony.gosselin/nuscenes', map_name='singapore-hollandvillage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_scene = nusc.scene[scene_num]  # {8: 1094,  0: 0061, 9: 1100}\n",
    "first_sample_token = my_scene['first_sample_token']\n",
    "my_sample = nusc.get('sample', first_sample_token)\n",
    "cam_front_data = nusc.get('sample_data', my_sample['data'][\"CAM_FRONT\"])\n",
    "front_camera_sensor = nusc.get('calibrated_sensor', cam_front_data['calibrated_sensor_token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_height(category: str):\n",
    "    heights_by_category = {\n",
    "        \"human.pedestrian.adult\": {\"mean\": 1.76, \"std\": 0.12},\n",
    "        \"human.pedestrian.child\":  {\"mean\": 1.37, \"std\": 0.06},\n",
    "        \"human.pedestrian.construction_worker\": {\"mean\": 1.78, \"std\": 0.05},\n",
    "        \"human.pedestrian.personal_mobility\": {\"mean\": 1.87, \"std\": 0.00},\n",
    "        \"human.pedestrian.police_officer\": {\"mean\": 1.81, \"std\": 0.00},\n",
    "        \"movable_object.barrier\": {\"mean\": 1.06, \"std\": 0.10},\n",
    "        \"movable_object.debris\":  {\"mean\": 0.46, \"std\": 0.00},\n",
    "        \"movable_object.pushable_pullable\": {\"mean\": 1.04, \"std\": 0.20},\n",
    "        \"movable_object.trafficcone\": {\"mean\": 0.78, \"std\": 0.13},\n",
    "        \"static_object.bicycle_rack\":  {\"mean\": 1.40, \"std\": 0.00},\n",
    "        \"vehicle.bicycle\":  {\"mean\": 1.39, \"std\": 0.34},\n",
    "        \"vehicle.bus.bendy\":  {\"mean\": 3.32, \"std\": 0.07},\n",
    "        \"vehicle.bus.rigid\":  {\"mean\": 3.80, \"std\": 0.62},\n",
    "        \"vehicle.car\": {\"mean\": 1.69, \"std\": 0.21},\n",
    "        \"vehicle.construction\":  {\"mean\": 2.38, \"std\": 0.33},\n",
    "        \"vehicle.motorcycle\":  {\"mean\": 1.47, \"std\": 0.20},\n",
    "        \"vehicle.trailer\":  {\"mean\": 3.71, \"std\": 0.27},\n",
    "        \"vehicle.truck\":  {\"mean\": 2.62, \"std\": 0.68},\n",
    "    }\n",
    "\n",
    "    stats = heights_by_category[category]\n",
    "    height = np.random.normal(stats['mean'], stats['std'])\n",
    "\n",
    "    return height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D bbox sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length: 9\n"
     ]
    }
   ],
   "source": [
    "bboxes_2d_seq = []\n",
    "ego_pose_seq = []\n",
    "ind = 0\n",
    "curr_sample = my_sample\n",
    "next_sample_token = curr_sample['token']\n",
    "\n",
    "MAX_FRAMES = 8\n",
    "\n",
    "while True:\n",
    "    curr_sample = nusc.get('sample', next_sample_token)\n",
    "    bboxes_2d_seq.append([])\n",
    "    \n",
    "    # The sequence of ego poses determines the trajectory of the camera through the reconstructed scene\n",
    "    # (This means we can move the camera as we please through the reconstructed scene)\n",
    "    curr_cam_front_data = nusc.get('sample_data', curr_sample['data'][\"CAM_FRONT\"])\n",
    "    ego_pose_seq.append(nusc.get('ego_pose', curr_cam_front_data['ego_pose_token']))\n",
    "\n",
    "    for ann_token in curr_sample['anns']:\n",
    "        bbox_3d = nusc.get_box(ann_token)\n",
    "        yaw, pitch, roll  = bbox_3d.orientation.yaw_pitch_roll\n",
    "        annotation_data = nusc.get('sample_annotation', ann_token)\n",
    "        bbox_2d = {'center': [bbox_3d.center[0], bbox_3d.center[1]], 'size': [bbox_3d.wlh[0], bbox_3d.wlh[1]], 'heading': yaw, 'category': bbox_3d.name, 'instance_token': annotation_data['instance_token'], 'height': bbox_3d.center[2]}\n",
    "        bboxes_2d_seq[ind].append(bbox_2d)\n",
    "    \n",
    "    next_sample_token = curr_sample['next']\n",
    "    if next_sample_token == '' or ind >= MAX_FRAMES:\n",
    "        break\n",
    "    ind += 1\n",
    "    \n",
    "print(\"Sequence length:\", len(bboxes_2d_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) With interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_data(prev_sample, next_sample, frequency):\n",
    "    \"\"\"\n",
    "    Interpolates both ego poses and bounding boxes for a given sample token at a specified frequency.\n",
    "    :param nusc: NuScenes instance.\n",
    "    :param sample_token: Token of the sample to interpolate.\n",
    "    :param frequency: Desired interpolation frequency in Hz.\n",
    "    :return: List of tuples (interpolated ego pose, interpolated bounding boxes).\n",
    "    \"\"\"\n",
    "\n",
    "    # Load ego poses for the previous and next samples\n",
    "    prev_cam_front_data = nusc.get('sample_data', prev_sample['data'][\"CAM_FRONT\"])\n",
    "    next_cam_front_data = nusc.get('sample_data', next_sample['data'][\"CAM_FRONT\"])\n",
    "\n",
    "    prev_ego_pose = nusc.get('ego_pose', prev_cam_front_data['ego_pose_token'])\n",
    "    next_ego_pose = nusc.get('ego_pose', next_cam_front_data['ego_pose_token'])\n",
    "\n",
    "    # Load annotations for the previous and next samples\n",
    "    prev_anns = {ann['instance_token']: ann for ann in map(lambda ann_token: nusc.get('sample_annotation', ann_token), prev_sample['anns'])}\n",
    "    next_anns = {ann['instance_token']: ann for ann in map(lambda ann_token: nusc.get('sample_annotation', ann_token), next_sample['anns'])}\n",
    "\n",
    "    # Calculate the time difference between samples\n",
    "    dt = (next_sample['timestamp'] - prev_sample['timestamp']) / 1e6  # Convert microseconds to seconds\n",
    "    steps = int(dt * frequency)  # Number of interpolation steps\n",
    "\n",
    "    # Interpolate the data\n",
    "    interpolated_boxes, interpolated_ego_poses, instance_tokens = [], [], []\n",
    "    for i in range(steps):\n",
    "        t = i / steps\n",
    "        \n",
    "        # Interpolate ego pose\n",
    "        translation = (1 - t) * np.array(prev_ego_pose['translation']) + t * np.array(next_ego_pose['translation'])\n",
    "        rotation = Quaternion.slerp(Quaternion(prev_ego_pose['rotation']), Quaternion(next_ego_pose['rotation']), t)\n",
    "        interpolated_ego_pose = {\n",
    "            'translation': translation,\n",
    "            'rotation': rotation\n",
    "        }\n",
    "        interpolated_ego_poses.append(interpolated_ego_pose)\n",
    "        \n",
    "        # Interpolate bounding boxes\n",
    "        interpolated_boxes.append([])\n",
    "        instance_tokens.append([])\n",
    "        for instance_token in set(prev_anns.keys()).union(next_anns.keys()):\n",
    "            if instance_token in prev_anns and instance_token in next_anns:\n",
    "                # Interpolate between the two annotations\n",
    "                prev_ann = prev_anns[instance_token]\n",
    "                next_ann = next_anns[instance_token]\n",
    "                center = (1 - t) * np.array(prev_ann['translation']) + t * np.array(next_ann['translation'])\n",
    "                size = (1 - t) * np.array(prev_ann['size']) + t * np.array(next_ann['size'])\n",
    "                orientation = Quaternion.slerp(Quaternion(prev_ann['rotation']), Quaternion(next_ann['rotation']), t)\n",
    "                box = Box(center=center, size=size, orientation=orientation, name=prev_ann['category_name'])\n",
    "                interpolated_boxes[i].append(box)\n",
    "                instance_tokens[i].append(instance_token)\n",
    "            elif instance_token in prev_anns:\n",
    "                # The bounding box disappears in the next sample\n",
    "                prev_ann = prev_anns[instance_token]\n",
    "                box = Box(center=prev_ann['translation'], size=prev_ann['size'], orientation=Quaternion(prev_ann['rotation']), name=prev_ann['category_name'])\n",
    "                interpolated_boxes[i].append(box)\n",
    "                instance_tokens[i].append(instance_token)\n",
    "            elif instance_token in next_anns:\n",
    "                # The bounding box appears in the next sample\n",
    "                next_ann = next_anns[instance_token]\n",
    "                box = Box(center=next_ann['translation'], size=next_ann['size'], orientation=Quaternion(next_ann['rotation']), name=next_ann['category_name'])\n",
    "                interpolated_boxes[i].append(box)\n",
    "                instance_tokens[i].append(instance_token)\n",
    "\n",
    "    return interpolated_boxes, interpolated_ego_poses, instance_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length: 117\n"
     ]
    }
   ],
   "source": [
    "bboxes_2d_seq = []\n",
    "ego_pose_seq = []\n",
    "ind = 0\n",
    "curr_sample = my_sample\n",
    "next_sample_token = curr_sample['token']\n",
    "interpolation_freq = 7  # Convert to 7Hz annotations\n",
    "\n",
    "MAX_FRAMES = 30\n",
    "\n",
    "while True:\n",
    "    curr_sample = nusc.get('sample', next_sample_token)\n",
    "\n",
    "    next_sample_token = curr_sample['next']\n",
    "    if next_sample_token == '' or ind >= MAX_FRAMES:\n",
    "        break \n",
    "    next_sample = nusc.get('sample', next_sample_token)\n",
    "        \n",
    "    interpolated_boxes, interpolated_ego_poses, instance_tokens = interpolate_data(curr_sample, next_sample, interpolation_freq)\n",
    "    for bboxes_3d_t, ego_pose, instance_tokens_t in zip(interpolated_boxes, interpolated_ego_poses, instance_tokens):\n",
    "\n",
    "        bboxes_2d_seq.append([])\n",
    "        \n",
    "        # The sequence of ego poses determines the trajectory of the camera through the reconstructed scene\n",
    "        # (This means we can move the camera as we please through the reconstructed scene)\n",
    "        ego_pose_seq.append(ego_pose)\n",
    "        \n",
    "        for bbox_idx, bbox_3d in enumerate(bboxes_3d_t):\n",
    "            yaw, pitch, roll  = bbox_3d.orientation.yaw_pitch_roll\n",
    "            \n",
    "            instance_token = instance_tokens_t[bbox_idx]\n",
    "            bbox_2d = {'center': [bbox_3d.center[0], bbox_3d.center[1]], \n",
    "                       'size': [bbox_3d.wlh[0], bbox_3d.wlh[1]], \n",
    "                       'heading': yaw, \n",
    "                       'category': bbox_3d.name, \n",
    "                       'instance_token': instance_token, \n",
    "                       'height': bbox_3d.center[2]}  # For testing...\n",
    "            bboxes_2d_seq[ind].append(bbox_2d)\n",
    "    \n",
    "        ind += 1\n",
    "    \n",
    "print(\"Sequence length:\", len(bboxes_2d_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D bbox sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class CVCOLORS:\n",
    "    RED = (0,0,255)\n",
    "    GREEN = (0,255,0)\n",
    "    BLUE = (255,0,0)\n",
    "    PURPLE = (247,44,200)\n",
    "    ORANGE = (44,162,247)\n",
    "    MINT = (239,255,66)\n",
    "    YELLOW = (2,255,250)\n",
    "    BROWN = (42,42,165)\n",
    "    LIME=(51,255,153)\n",
    "    GRAY=(128, 128, 128)\n",
    "    LIGHTPINK = (222,209,255)\n",
    "    LIGHTGREEN = (204,255,204)\n",
    "    LIGHTBLUE = (255,235,207)\n",
    "    LIGHTPURPLE = (255,153,204)\n",
    "    LIGHTRED = (204,204,255)\n",
    "    WHITE = (255,255,255)\n",
    "    BLACK = (0,0,0)\n",
    "    \n",
    "    TRACKID_LOOKUP = defaultdict(lambda: (np.random.randint(50, 255), np.random.randint(50, 255), np.random.randint(50, 255)))\n",
    "    TYPE_LOOKUP = [BLUE, WHITE, RED, YELLOW, PURPLE, BROWN, GREEN, ORANGE, LIGHTPURPLE, LIGHTRED, GRAY]\n",
    "    REVERT_CHANNEL_F = lambda x: (x[2], x[1], x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on closest match to KITTI classes\n",
    "NUSC_CLASS_TO_GROUP_IDS_KITTI = {\n",
    "    \"human.pedestrian.adult\": 4,\n",
    "    \"human.pedestrian.child\":  4,\n",
    "    \"human.pedestrian.construction_worker\": 5,\n",
    "    \"human.pedestrian.personal_mobility\": 4,\n",
    "    \"human.pedestrian.police_officer\": 5,\n",
    "    \"movable_object.barrier\": 8,\n",
    "    \"movable_object.debris\":  8,\n",
    "    \"movable_object.pushable_pullable\": 8,\n",
    "    \"movable_object.trafficcone\": 8,\n",
    "    \"static_object.bicycle_rack\":  8,\n",
    "    \"vehicle.bicycle\":  6,\n",
    "    \"vehicle.bus.bendy\":  3,\n",
    "    \"vehicle.bus.rigid\":  3,\n",
    "    \"vehicle.car\": 1,\n",
    "    \"vehicle.construction\":  3,\n",
    "    \"vehicle.motorcycle\":  6,  # NOTE: Not sure if best to classify as cyclist or car...\n",
    "    \"vehicle.trailer\":  3,\n",
    "    \"vehicle.truck\":  3,\n",
    "}\n",
    "\n",
    "# KITTI:\n",
    "# IDS_CLASS_LOOKUP = {\n",
    "#     1: 'Car',\n",
    "#     2: 'Van',\n",
    "#     3: 'Truck',\n",
    "#     4: 'Pedestrian',\n",
    "#     5: 'Person',\n",
    "#     6: 'Cyclist',\n",
    "#     7: 'Tram',\n",
    "#     8: 'Misc',\n",
    "#     9: 'DontCare'\n",
    "# }\n",
    "\n",
    "# Based on closest match to BDD100k classes\n",
    "NUSC_CLASS_TO_GROUP_IDS_BDD = {\n",
    "    \"human.pedestrian.adult\": 1,\n",
    "    \"human.pedestrian.child\":  1,\n",
    "    \"human.pedestrian.construction_worker\": 1,\n",
    "    \"human.pedestrian.personal_mobility\": 1,\n",
    "    \"human.pedestrian.police_officer\": 1,\n",
    "    \"movable_object.barrier\": 10,\n",
    "    \"movable_object.debris\":  10,\n",
    "    \"movable_object.pushable_pullable\": 10,\n",
    "    \"movable_object.trafficcone\": 10,\n",
    "    \"static_object.bicycle_rack\":  10,\n",
    "    \"vehicle.bicycle\":  8,\n",
    "    \"vehicle.bus.bendy\":  5,\n",
    "    \"vehicle.bus.rigid\":  5,\n",
    "    \"vehicle.car\": 3,\n",
    "    \"vehicle.construction\":  4,\n",
    "    \"vehicle.motorcycle\":  7, \n",
    "    \"vehicle.trailer\":  4,\n",
    "    \"vehicle.truck\":  4,\n",
    "}\n",
    "\n",
    "# BDD100k:\n",
    "# IDS_CLASS_LOOKUP = {\n",
    "#         1: 'pedestrian',\n",
    "#         2: 'rider',\n",
    "#         3: 'car',\n",
    "#         4: 'truck',\n",
    "#         5: 'bus',\n",
    "#         6: 'train',\n",
    "#         7: 'motorcycle',\n",
    "#         8: 'bicycle',\n",
    "#         9: 'traffic light',\n",
    "#         10: 'traffic sign',\n",
    "#     }\n",
    "\n",
    "NUSC_CLASS_TO_GROUP_IDS = NUSC_CLASS_TO_GROUP_IDS_BDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_box_3d_style_CV(box, img, canvas3d, canvas2d, view: np.ndarray = np.eye(3), normalize: bool = False, outline_color=(255, 0, 0), fill_color=(0, 255, 0), linewidth: float = 2, show_3d_bboxes=True, show_2d_bboxes=False) -> None:\n",
    "    \"\"\"\n",
    "    Renders the box in the provided canvas\n",
    "    \"\"\"\n",
    "    corners = view_points(box.corners(), view, normalize=normalize)[:2, :]\n",
    "    corners = np.round(corners).astype(int)\n",
    "\n",
    "    if show_3d_bboxes:\n",
    "        def draw_rect(selected_corners, color):\n",
    "            prev = selected_corners[-1]\n",
    "            for corner in selected_corners:\n",
    "                cv2.line(canvas3d, (prev[0], prev[1]), (corner[0], corner[1]), color=color, thickness=linewidth)\n",
    "                prev = corner\n",
    "\n",
    "        # Draw front (first 4 corners) and rear (last 4 corners) rectangles(3d)/lines(2d)\n",
    "        draw_rect(corners.T[:4], outline_color)\n",
    "        draw_rect(corners.T[4:], outline_color)\n",
    "\n",
    "        # Draw the sides\n",
    "        for i in range(4):\n",
    "            cv2.line(canvas3d, (corners.T[i][0], corners.T[i][1]),\n",
    "                    (corners.T[i + 4][0], corners.T[i + 4][1]),\n",
    "                    color=outline_color, thickness=linewidth)\n",
    "        \n",
    "        # Draw x mark at the back of the object\n",
    "        cv2.line(canvas3d, [corners.T[4][0], corners.T[4][1]],\n",
    "                [corners.T[6][0], corners.T[6][1]],\n",
    "                color=outline_color, thickness=1)\n",
    "        cv2.line(canvas3d, [corners.T[5][0], corners.T[5][1]],\n",
    "                [corners.T[7][0], corners.T[7][1]],\n",
    "                color=outline_color, thickness=1)\n",
    "\n",
    "    if show_2d_bboxes:\n",
    "        # Calculate the bottom left corner of the rectangle\n",
    "        bottom_left_x, top_right_x = np.min(corners.T[:, 0]), np.max(corners.T[:, 0])\n",
    "        bottom_left_y, top_right_y = np.min(corners.T[:, 1]), np.max(corners.T[:, 1])\n",
    "\n",
    "        # Create the rectangle\n",
    "        cv2.rectangle(canvas2d, (bottom_left_x, bottom_left_y), (top_right_x, top_right_y), color=fill_color, thickness=cv2.FILLED)\n",
    "        if not show_3d_bboxes:\n",
    "            cv2.rectangle(canvas2d, (bottom_left_x, bottom_left_y), (top_right_x, top_right_y), color=outline_color, thickness=2)\n",
    "\n",
    "    alpha_2dbbox = 0.75\n",
    "    mask = canvas2d.astype(bool)\n",
    "    img[mask] = cv2.addWeighted(canvas2d, alpha_2dbbox, img, 1-alpha_2dbbox, 0)[mask]\n",
    "    mask = canvas3d.astype(bool)\n",
    "    img[mask] = canvas3d[mask]\n",
    "    \n",
    "    return img\n",
    "            \n",
    "        \n",
    "def my_render_3d_style_CV(nusc, boxes_3d, camera_sensor, ego_pose, data_path=None, transform=True, background=False, show_3d_bboxes=True, show_2d_bboxes=False) -> None:\n",
    "    \"\"\"\n",
    "    Bboxes are to be in global coordinate frame, and will be projected to the specified camera\n",
    "    \"\"\"\n",
    "    \n",
    "    im_size = (1600, 900)\n",
    "    img = torch.zeros((3, im_size[1], im_size[0]))\n",
    "    img = img.permute((1, 2, 0)).detach().cpu().numpy().copy()*255\n",
    "    img = img.astype(np.uint8)\n",
    "    canvas3d = np.zeros_like(img)\n",
    "    canvas2d = np.zeros_like(img)\n",
    "    \n",
    "    # Camera extrinsic and intrinsic parameters\n",
    "    camera_intrinsic = np.array(camera_sensor['camera_intrinsic'])\n",
    "\n",
    "    for ind, box_3d in enumerate(boxes_3d):\n",
    "        \n",
    "        if transform:\n",
    "            # Move box to ego vehicle coord system.\n",
    "            box_3d.translate(-np.array(ego_pose['translation']))\n",
    "            box_3d.rotate(Quaternion(ego_pose['rotation']).inverse)\n",
    "            #  Move box to sensor coord system.\n",
    "            box_3d.translate(-np.array(camera_sensor['translation']))\n",
    "            box_3d.rotate(Quaternion(camera_sensor['rotation']).inverse)\n",
    "\n",
    "        # Only render bboxes that fit in image frame\n",
    "        if not box_in_image(box_3d, camera_intrinsic, im_size, vis_level=1):\n",
    "            continue\n",
    "            \n",
    "        # outline_color = np.array(CVCOLORS.REVERT_CHANNEL_F(CVCOLORS.TYPE_LOOKUP[NUSC_CLASS_TO_GROUP_IDS[box_3d.name]])) / 255.0\n",
    "        outline_color = CVCOLORS.REVERT_CHANNEL_F(CVCOLORS.TYPE_LOOKUP[NUSC_CLASS_TO_GROUP_IDS[box_3d.name]])\n",
    "        instance_token = box_3d.token\n",
    "        if not transform:\n",
    "            annotation_data = nusc.get('sample_annotation', box_3d.token)\n",
    "            instance_token = annotation_data['instance_token']\n",
    "        # fill_color = np.array(CVCOLORS.REVERT_CHANNEL_F(CVCOLORS.TRACKID_LOOKUP[instance_token])) / 255.0\n",
    "        fill_color = CVCOLORS.REVERT_CHANNEL_F(CVCOLORS.TRACKID_LOOKUP[instance_token])\n",
    "        \n",
    "        img = render_box_3d_style_CV(box_3d, img, canvas3d, canvas2d, view=camera_intrinsic, normalize=True, outline_color=outline_color, fill_color=fill_color, show_3d_bboxes=show_3d_bboxes, show_2d_bboxes=show_2d_bboxes)\n",
    "        # if \"personal_mobility\" in box_3d.name:\n",
    "        #     print(box_3d.name, instance_token)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Create video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved: video_out/scene-1094_style.avi\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "ego_height = get_random_height('vehicle.car')\n",
    "front_cam_token = my_sample['data']['CAM_FRONT']\n",
    "agent_heights = {}\n",
    "\n",
    "# Parameters for the video\n",
    "video_filename = f\"video_out/{my_scene['name']}_style.avi\"\n",
    "FPS = 7 # NOTE Real time is 2 fps\n",
    "frame_size = (512, 320)\n",
    "\n",
    "transform=transforms.Compose([\n",
    "                    transforms.Resize((frame_size[1], frame_size[0])),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)), # map from [0,1] to [-1,1]\n",
    "                 ])\n",
    "\n",
    "# Create a VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "video_writer_out = cv2.VideoWriter(video_filename, fourcc, FPS, frame_size)\n",
    "\n",
    "img_seq = torch.zeros([len(bboxes_2d_seq), 3, frame_size[1], frame_size[0]])\n",
    "\n",
    "for t, bboxes_2d_t in enumerate(bboxes_2d_seq):\n",
    "    bboxes_3d_out = []\n",
    "    for ind, bbox_2d in enumerate(bboxes_2d_t):\n",
    "        center = [bbox_2d['center'][0], bbox_2d['center'][1], ego_height/2] # Adjust height because sensor is mounted on ego (And we don't have z-height information)\n",
    "        \n",
    "        agent_height = agent_heights.get(bbox_2d['instance_token'])  # OR Gt height information: bbox_2d['height'] \n",
    "        if agent_height is None:\n",
    "            agent_heights[bbox_2d['instance_token']] = get_random_height(bbox_2d['category'])\n",
    "            agent_height = agent_heights.get(bbox_2d['instance_token'])\n",
    "                       \n",
    "        size = [bbox_2d['size'][0], bbox_2d['size'][1], agent_height]\n",
    "        orientation = Quaternion._from_axis_angle(np.array([0, 0, 1]), bbox_2d['heading'])\n",
    "        bbox_3d = Box(center, size, orientation, name=bbox_2d['category'], token=bbox_2d['instance_token'])\n",
    "        bboxes_3d_out.append(bbox_3d)\n",
    "\n",
    "    # Update ego pose (else the POV will remain fixed in the scene)\n",
    "    curr_ego_pose = ego_pose_seq[t]\n",
    "\n",
    "    img = my_render_3d_style_CV(nusc, bboxes_3d_out, front_camera_sensor, curr_ego_pose, data_path=None, background=True, show_2d_bboxes=True, show_3d_bboxes=False)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Save to tensor buffer\n",
    "    pil_image = Image.fromarray(img)\n",
    "    img_seq[t] = transform(pil_image)\n",
    "\n",
    "    img = cv2.resize(img, frame_size)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    video_writer_out.write(img)\n",
    "    \n",
    "video_writer_out.release()\n",
    "print(f\"Video saved: {video_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create GT video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import transforms\n",
    "# import torch\n",
    "\n",
    "# ego_height = get_random_height('vehicle.car')\n",
    "# front_cam_token = my_sample['data']['CAM_FRONT']\n",
    "# agent_heights = {}\n",
    "\n",
    "# # Parameters for the video\n",
    "# video_filename = f\"video_out/{my_scene['name']}_style_gt.avi\"\n",
    "# FPS = 2  # NOTE Real time is 2 fps\n",
    "# frame_size = (512, 320)\n",
    "\n",
    "# transform=transforms.Compose([\n",
    "#                     transforms.Resize((frame_size[1], frame_size[0])),\n",
    "#                     transforms.ToTensor(),\n",
    "#                     transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)), # map from [0,1] to [-1,1]\n",
    "#                  ])\n",
    "\n",
    "# # Create a VideoWriter object\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "# video_writer_out = cv2.VideoWriter(video_filename, fourcc, FPS, frame_size)\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(9, 9))\n",
    "# plt.tight_layout(pad=0)\n",
    "\n",
    "# # img_seq_gt = torch.zeros([len(bboxes_2d_seq), 3, frame_size[1], frame_size[0]])\n",
    "\n",
    "# curr_sample = my_sample\n",
    "# next_sample_token = curr_sample['token']\n",
    "\n",
    "# t = 0\n",
    "# while True:\n",
    "#     curr_sample = nusc.get('sample', next_sample_token)\n",
    "\n",
    "#     # fig, ax = plt.subplots(1, 1, figsize=(9, 9))\n",
    "#     # plt.tight_layout(pad=0)\n",
    "\n",
    "#     ax.clear()\n",
    "#     data_path, bboxes_3d_local, camera_intrinsic = nusc.get_sample_data(curr_sample['data']['CAM_FRONT'], selected_anntokens=curr_sample['anns'])\n",
    "#     img = my_render_3d_style_CV(nusc, bboxes_3d_local, front_camera_sensor, ego_pose, data_path=data_path, transform=False, background=True, show_2d_bboxes=True, show_3d_bboxes=False)\n",
    "\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#     img = cv2.resize(img, frame_size)\n",
    "\n",
    "#     # Save to tensor buffer\n",
    "#     pil_image = Image.fromarray(img)\n",
    "#     img_seq[t] = transform(pil_image)\n",
    "\n",
    "#     # img_seq_gt[t] = transform(Image.fromarray(img))\n",
    "#     # t += 1\n",
    "\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "#     video_writer_out.write(img)\n",
    "\n",
    "#     # plt.show(fig)\n",
    "\n",
    "#     next_sample_token = curr_sample['next']\n",
    "#     if next_sample_token == '':\n",
    "#         break\n",
    "\n",
    "    \n",
    "# video_writer_out.release()\n",
    "# print(f\"Video saved: {video_filename}\")\n",
    "# plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ctrl-V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    from ctrlv.utils import export_to_video\n",
    "    from ctrlv.pipelines import StableVideoControlPipeline\n",
    "\n",
    "OUT_DIR = \"/network/scratch/x/xuolga/Results/sd3d/bdd100k_ctrlv_240511_200727/\" #kitti_ctrlv_240510_141159/\" # This one exists: kitti_ctrlv_240513_195113/ \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# set_seed(args.seed)\n",
    "generator = None #torch.Generator(device=accelerator.device).manual_seed(args.seed) if args.seed else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/a/anthony.gosselin/.conda/envs/merge/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading pipeline components...: 100%|██████████| 5/5 [00:00<00:00,  7.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load pipelines\n",
    "from ctrlv.models import UNetSpatioTemporalConditionModel, ControlNetModel\n",
    "ctrlnet = ControlNetModel.from_pretrained(OUT_DIR, subfolder=\"controlnet\")\n",
    "unet = UNetSpatioTemporalConditionModel.from_pretrained(OUT_DIR, subfolder=\"unet\")\n",
    "pipeline = StableVideoControlPipeline.from_pretrained(\"stabilityai/stable-video-diffusion-img2vid-xt\", controlnet = ctrlnet, unet = unet,)\n",
    "pipeline = pipeline.to(device)\n",
    "pipeline.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Generate video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start inference...\n"
     ]
    }
   ],
   "source": [
    "# Get initial image for conditioning\n",
    "image_init_path = os.path.join(nusc.dataroot, cam_front_data['filename'])\n",
    "image_init = Image.open(image_init_path)\n",
    "sample_out = {'bbox_img': img_seq, 'image_init': image_init}\n",
    "\n",
    "# sample_out = {'bbox_img': img_seq_gt, 'image_init': image_init} \n",
    "\n",
    "\n",
    "CLIP_LENGTH = sample_out['bbox_img'].shape[0]\n",
    "\n",
    "\n",
    "def run_inference_with_pipeline(pipeline, demo_samples):\n",
    "    for sample_i, sample in enumerate(demo_samples):\n",
    "        frames = pipeline(sample['image_init'], \n",
    "                        cond_images=sample['bbox_img'].unsqueeze(0),\n",
    "                        height=frame_size[1], width=frame_size[0], \n",
    "                        decode_chunk_size=8, motion_bucket_id=127, fps=FPS, \n",
    "                        num_inference_steps=30,\n",
    "                        num_frames=CLIP_LENGTH,\n",
    "                        control_condition_scale=1.0,\n",
    "                        min_guidance_scale=1.0,\n",
    "                        max_guidance_scale=3.0,\n",
    "                        noise_aug_strength=0.01,\n",
    "                        generator=generator, output_type='pt').frames[0]\n",
    "        #frames = F.interpolate(frames, (dataset.orig_H, dataset.orig_W)).detach().cpu().numpy()*255\n",
    "        frames = frames.detach().cpu().numpy()*255\n",
    "        frames = frames.astype(np.uint8)\n",
    "\n",
    "        tmp = np.moveaxis(np.transpose(frames, (0, 2, 3, 1)), 0, 0)\n",
    "        output_video_path = f\"video_out/generated_ctrl_{my_scene['name']}.mp4\"\n",
    "        export_to_video(tmp, output_video_path, fps=FPS)\n",
    "        print(f\"Video saved:\", output_video_path)\n",
    "        # log_dict = {}\n",
    "        # log_dict[\"generated_videos\"] = wandb.Video(frames, fps=args.fps)\n",
    "        # log_dict[\"gt_bbox_frames\"] = wandb.Video(sample['bbox_img_np'], fps=args.fps)\n",
    "        # log_dict[\"gt_videos\"] = wandb.Video(sample['gt_clip_np'], fps=args.fps)\n",
    "        # frame_bboxes = wandb_frames_with_bbox(frames, sample['objects_tensors'], (dataset.orig_W, dataset.orig_H))\n",
    "        # log_dict[\"frames_with_bboxes_{}\".format(sample_i)] = frame_bboxes\n",
    "\n",
    "        \n",
    "print(\"Start inference...\")\n",
    "sample_out['bbox_img'].to(device)\n",
    "run_inference_with_pipeline(pipeline, [sample_out])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
